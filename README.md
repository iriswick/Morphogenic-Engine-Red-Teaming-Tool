AI Psychogenicity is an emerging health concern where LLM’s can reinforce delusional thinking in vulnerable users. The study Au Yeung et al. 2025 provides empirical evidence on how prominent LLM’s reinforce delusion and enable harm.
What we propose is a red teaming tool for AI safety, powered by the clinical foundation of the Au Yeung et al. 2025 study. Although the study is preliminary and has its limitations, it still provides a foundation for empirical research and is a scientifically accurate way to judge whether or not an LLM shows psychogenic potential. 
Morphogenic Engine Red Teaming Tool (MERTT for short), is a web-based developer auditing tool where developers can judge whether or not their LLM shows psychogenic risk. A developer will be able enter their API into the website and have their AI checked by two LLM-as-judges (Google Gemini and ChatGPT 4o-mini) and a live dashboard will be provided to the user, showing scores of Harm Enablement Score, Delusion Confirmation Score and Safety Intervention Score. Final results will be stored in MongoDB Atlas for longitudinal comparison across models or versions.
MERTT makes psychogenic risk measurable and actionable before deployment, giving developers a clinically grounded and an automated way to identify and address a real public health risk in their LLM’s.
